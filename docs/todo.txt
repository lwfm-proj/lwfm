
Arch: Parsl 
    - parsl deployed on every HPC node 
    - parsl on head node doles out tasks to nodes based on data flow 
    - python and bash apps supported
    - has own launcher / executor model - PSI/J driver coming soon 
    - distributed launchers (on every node?) vs lwfm centrally scaled 

Arch: PSI/J
    - JobSpec: cmd line, args, stdout & err 
    - JobAttributes: project id, etc.
    - ResourceSpec: nodes, processes
    - Executor: e.g. slurm 
    - Launcher: e.g. mpirun
    - Job: attach JobSpec & ResourceSpec, set launcher, submit & wait or set job status callback python func
    - Job Status: state (https://exaworks.org/psij-python/docs/v/0.9.3/.generated/tree.html#psij.JobState) + history
    - Job State = enum 
    - ProcessReaper - polls processes being tracked to completion, uses nodelist file to track deleted at end of run; starting the executor
        starts the singleton reaper 

Arch: lwfm 
    - morph triggers into futures, complex joins, retries  
    - inc2 methods (C++ & Python) for Site interface - inc2 job can notate, initiate new jobs - 4 pillar verbs 
    - generic Site driver for psi/j 
    - arch doc: middleware deployment for 3 wf types (1 head node only, 2 enterprise or local, 3 enterprise or local)
    - arch doc: format stds in ref model - wf interop, metadata, job spec, site spec, job id & status 
    - cross-site scheduler would use job & site metadata to match jobs 
    - fire & forget notation 

Arch: General Model
    * Run
	    - Workflow representation / spec / interface - static vs. dynamic (iterative versioned static
		    variations per user audiences (production & consumption) - LLM xform 
	    -  Workflow execution / runtime / interface - static vs. dynamic; futures-oriented - PSI/J -
		    futures programming as "intent-based" with verification 
	    - top-down vs. bottom up 
	    - planner / executor / debugger (visualizer spans all) 

    * Repo 
	    - record metadata at time of production (in situ) - fire & forget - domain specific AI assist on
 		    decoration?  (fairdo)
	    - data staging tasks (see parsl w taskvine) 


* * * * * * * * * * * * * 

PSI/J  
    - make an Executor (e.g. slurm)
    - make a JobSpec (executable, args, in/out)
    - make ResourceSpec (e.g. #nodes)
    - make a Job - set the JobSpec, ResourceSpec, launcher (e.g. mpirun)
    - submit the job 
    - sit on job.wait() 

lwfm 
    - make a Site with a Run subsystem 
    - make a JobDefn (executable)
    - make a JobContext (id)
    - submit the job 
    - poll for completion status 

* psij
    - JobSpec: executable, args, run directory, display name, environment vars, std in out & err paths, handle to ResourceSpec, 
        attributes (e.g. wall time), pre-launch script, post-launch script, handle to Launcher (e.g. mpirun) - there's an assumption here
        that there's a unix-like OS here, so its not "site specific" because only unix-like sites are supported 
    - ResourceSpec: #node, #proc, #gpu, etc. - (compute site specific)
    - Job: id, native id, handle to JobSpec, handle to on status change callback, handle to JobSpec, JobStatus (with own canonical states: 
        new, queued, active, completed, cancelled, failed, ), methods: set callback, wait, cancel

* lwfm 
    - JobDefn: display name, compute type ("resource", can also represent a sub-Site), 
        entry point ("executable", but not necessarily a path to an exe... we permit any 
        struct here and let Site.Run decide how to handle), 
        job args (any args to be interpreted by the Site.Run - different Sites might use these differently...) - 
        thus the JobDefn, like psij.ResourceSpec, is potentially Site specific 
    - RepoJobDefn: subclass of JobDefn to support data movement jobs 
    - JobStatus: job id, native job id, has a JobContext with parent & originator job ids, ref to to the site which produced this status
    - lwfm does not have its own "Job" class specifically - the Job fields from psij are in the lwfm.JobStatus. 
        psij also adds the status callback (github issue list suggests psij.Job.wait() doesn't work for all states)


(base) :~/src/lwfm-proj$ . lwfm/dev/start-mw.sh 


A -> B -> C use case (pseudocode)
    - synchronous: make & run A and wait for it, then repeat for B and then C
            # run job B after job A
            jobA = makeJob(A)
            jobA.submit()
            jobA.wait()
            # repeat for B and C
            jobB = makeJob(B)  etc.
    - asynchronous control flow: downstream jobs B and C may of may not have control and/or data flow dependencies; if 
        they have no control flow dependency, they can be run simultaneous.  if they have a control flow 
        dependency A triggers B.  
            jobA = makeJob(A)
            jobB = makeJob(B)
            setJobTrigger(jobB, jobA)
            jobA.submit()
    - asynchronous join: same as a control flow dependency, but B is dependent on a set of upstream A 
            jobA1 = makeJob(A)
            jobA2 = makeJob(A)
            jobB = makeJob(B)
            setJobTrigger(jobB, [jobA1, jobA2])
            jobA1.submit()
            jobA2.submit()
    - asynchronous data flow: if two jobs have a data flow dependency, the creation of data with a metadata
        signature (potenially but not necessarily by job A) triggers the running of B.
            


















* * * * * * * * * * * * * 

Questions: 
    - how important is long-term reproducibility vs. (meta)data storage 
    - nouns / verbs / building blocks backing up NLP wf constructs (wf fragments, w specific required input vars to be teased out) - "teach" them using what training corpus?
    - type 1, 2, 3 lens / unification 
    - multi-tool, multi-processor, multi-host, multi-lang fw, multi-lang



Middleware 
    * Type 1
        - MPMD controller (Python / C++) colaunches app(s) within the allocation, uses lwfm lib to (auth), 
            run other jobs via a site scheduler
        - if the controller is running a portable scheduler (e.g. flux) then the type 1 can use the lwfm lib
            to launch jobs vs a site which is the allocation itself
        - where is the middleware?  when does the controller need its own?  when does it use the invoking site's 
            type 2 middleware 
    * Type 2
        - enterprise middleware
    * Type 3 
        - personal middleware vs. enterprise middle running against permitted external sites - see auth, etc.


Use Cases

    1. lwfm middleware is deployed to a node within the enterprise and is scaled to support the size of the compute
        cluster (number of nodes, frequency of hits).  it supports jobs running on all HPC and other enterprise 
        servers.  
        
    1a. the enterprise middleware can be used for local user jobs 
    
    1b. the enterprise middleware can be used for type 3 workflows initiated from within an enterprise type 2 workflow.
    
    1c. the enterprise middleware can be used for type 3 workflows initiated locallty by the user.
    
    1d. type 3 workflows in which the remote job may call back on the enterprise middleware are permitted.
    
    1e. type 3 workflows in which the remote job may not call back on the enterprise middleware are only permitted.  here the remote site may have its own copy of the middleware, thus import/export is utilized.


    2. lwfm middleware is deployed by the user in the scope of user and other compute servers as if they were 
        their own enterprise - see use case 1.

    lwfm middleware can be deployed in lightweight singleton user or process-launched instances, or scaled 
    enterprise instances

    middleware: metarepo, event handler 
    interfaces: sdk, gui



C++ interface should have Repo method for fire & forget output annotion, Run interface to launch other jobs vs. scheduler, etc.


data movement jobs
jobs decorated with requirements, sites decorated with capabilities 


* * * * * * * * * * * * 

JobStatus contains a discrete state from an enumeration of states (pending, cancelled, etc.).  The state changes over time
and each change is noted at a certain timestamp - one when the state change happened in the compute environment, and 
another timestamp when the state change is received by the middleware. There is therefore a history in time of those state
transitions.  The state enumeration is from the canonical lwfm states, so there's also (for non-"local" jobs) a native 
state, and its the duty of the Site.Run interface to perform that mapping between states.  

The JobStatus also includes a set of fields grouped as a JobContext.  These include the job id assigned when the job
was submitted to the runtime (Site.Run), and the lwfm id has a corresponding native runtime job id.  It has the lwfm 
job ids of this job's parent job, and the originator job of the whole workflow.  There is a job display name, and the 
name of the Site on which the job is queued.  If the Site exposes a specific named compute type (a specific configuration of 
a computing resource) it is noted here.  The JobContext is not reusable, and a JobStatus once emitted it immutable.  

A JobDefn defines the static job - what will be executed when the job is run, for example, "run ansys with certain 
arguments".  And it names a symbolic resouce - a compute type - on which the job must run.  An arbitrary args list is 
supported. A subclass of JobDefn is the RepoJobDefn which is a convenience mechansim for data movement jobs.

Thus in lwfm a "job" has two representations - one static (the JobDefn) and one dynamic (the JobContext with its running
list of JobStatus).

In PSI/J, a JobSpec is like an lwfm JobDefn - it contains the name of the executable, args, the run directory, display name, 
environment vars, std in out & err paths, a pre-launch script, a post-launch script, and other runtime boundary attributes 
such as duration (wall time), project name (for accounting), and a catch-all generic arg list.  There is a sub-object, the ResourceSpec, which defines the number of required compute nodes, how many GPUs, etc.  And it defines the MPI launcher (e.g. mpirun).  Thus the PSI/J JobSpec is site-specific - the launcher is site specific, as is the request for a specific node and GPU configuration.

Comparing PSI/J JobSpec to the lwfm JobDefn, they are conceptually the same, with the lwfm arbitrary args list encompassing all the PSI/J args.  The lwfm compute type can be used as a symbolic name representing a set of attributes and their values - the Site can expose this as a specific named configuration, in addition to permitting user-defined arbitrary runtime 
configurations. The PSI/J adds specific named fields for things like number of compute nodes, and it tacks down the launcher syntax.  lwfm does not expose the launcher syntax to the user - that is the purvue of the Site.Run implementation, which of course is Site-specific, but it frees the lwfm JobDefn be mnore portable.  

In PSI/J, the Job object contains what in lwfm is the JobContext - the canonical and native job ids - and the lwfm JobStatus - the canonical job state.  The PSI/J Job object includes handles to the other PSI/J constructs such as JobSpec, but also includes a per-Job user-setable "on state change callback" method.  A callback of this kind might, for example, launch the next job in some A -> B control flow sequence.  The callback can also be used for error handling.  In lwfm, these functions are handled by the Event Handler middleware component and the interface to it exposed in Site.Run - a JobDefm can be set to fire on the state change of some other job's execution and state changes.  

PSI/J only represents the Run portion of the lwfm Site model.  Thus bolting lwfm to PSI/J would be a political activity.  However, bolting PSI/J under the hood of lwfm is entirely line-of-sight - the concepts map almost verbatim, and a Site.Run subsystem implemented with PSI/J makes a nice building block on which to model many real-world sites which use the runtime schedulers supported by PSI/J.  

As part of this investigation we also re-examined Parsl, a Python library with annotations for distributed applications which are written in either Python itself, or bash.  Parsl takes a data-flow centric approach, and rather than implement an independently scalable middle tier like lwfm, Parsl distributes task management responsibility to every compute node in the allocation.  There is significant overlap in the runtime model of both packages and they originate from the same community in and around Chicago.  The detailed notes of the Parsl project suggest that its a roadmap item to fold PSI/J under it - again, PSI/J by itself is only part of the functionality needed by site-distributed HPC apps.  

We also looked at Mojo as an alternative to Python, the former promising backward compatibility to Python with significant speed up, perhaps being useable in both computations and workflows.  Its simply not ready for prime time (e.g. doesn't implement kwargs).  Nor is Julia ready for programming end-to-end jobs, with the added problem that it requires code to be rewritten.  We note that work at Argonne to utilize an LLM to aid in workflow and workflow fragment programming first took a workflow expressed in Common Worflow Language mark-up, and rewrote it using Python before trying to present it to an LLM.  


* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 

TODO - KEEP 


+ now
    - test loggger in dt4d example
    - compute type in status message via job context (anything else to do?)
    - listComputeTypes returns complex list object w. metadata

lwfm: Repo.Run job status serialization incl. subclasses & job context

+ priority 1
    + event handlers
        - metadata-based filter implementation & example
        - better examples around multi-site - not all sites implement, but local does
        - triggers should work like >= as remote sites might not report all states
        - test: trigger job running on remote site, targer runs on remote site
    - SiteFileRef with timestamp (see TODO)
    - MetaRepo notate()


+ priority 2
    - JobStatus history
    - multi-Site Job status panel -> similar to DT4D's, but multi-Site
    - demo: lab device as a Site
    + event handlers
        - persisting over lwfm restart
        - full trigger model impl - fuzzy, timeouts, wildcards, persistence, etc.


+ priority 3
    - local site driver subclass with ssh as run, scp as repo, with auth credentials
    - digital thread graph render & navigate, incl. event trigger futures
    - RunRepo - robust impl
    - JSS security
    - adaptive job status polling based on emit & received job status timestamps




