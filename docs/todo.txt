
Arch: Parsl 
    - parsl deployed on every HPC node 
    - parsl on head node doles out tasks to nodes based on data flow 
    - python and bash apps supported
    - has own launcher / executor model - PSI/J driver coming soon 
    - distributed launchers (on every node?) vs lwfm centrally scaled 

Arch: PSI/J
    - JobSpec: cmd line, args, stdout & err 
    - JobAttributes: project id, etc.
    - ResourceSpec: nodes, processes
    - Executor: e.g. slurm 
    - Launcher: e.g. mpirun
    - Job: attach JobSpec & ResourceSpec, set launcher, submit & wait or set job status callback python func
    - Job Status: state (https://exaworks.org/psij-python/docs/v/0.9.3/.generated/tree.html#psij.JobState) + history
    - Job State = enum 
    - ProcessReaper - polls processes being tracked to completion, uses nodelist file to track deleted at end of run; starting the executor
        starts the singleton reaper 

Arch: lwfm 
    - morph triggers into futures, complex joins, retries  
    - inc2 methods (C++ & Python) for Site interface - inc2 job can notate, initiate new jobs - 4 pillar verbs 
    - generic Site driver for psi/j 
    - arch doc: middleware deployment for 3 wf types (1 head node only, 2 enterprise or local, 3 enterprise or local)
    - arch doc: format stds in ref model - wf interop, metadata, job spec, site spec, job id & status 
    - cross-site scheduler would use job & site metadata to match jobs 
    - fire & forget notation 

Arch: General Model
    * Run
	    - Workflow representation / spec / interface - static vs. dynamic (iterative versioned static
		    variations per user audiences (production & consumption) - LLM xform 
	    -  Workflow execution / runtime / interface - static vs. dynamic; futures-oriented - PSI/J -
		    futures programming as "intent-based" with verification 
	    - top-down vs. bottom up 
	    - planner / executor / debugger (visualizer spans all) 

    * Repo 
	    - record metadata at time of production (in situ) - fire & forget - domain specific AI assist on
 		    decoration?  (fairdo)
	    - data staging tasks (see parsl w taskvine) 


* * * * * * * * * * * * * 

PSI/J  
    - make an Executor (e.g. slurm)
    - make a JobSpec (executable, args, in/out)
    - make ResourceSpec (e.g. #nodes)
    - make a Job - set the JobSpec, ResourceSpec, launcher (e.g. mpirun)
    - submit the job 
    - sit on job.wait() 

lwfm 
    - make a Site with a Run subsystem 
    - make a JobDefn (executable)
    - make a JobContext (id)
    - submit the job 
    - poll for completion status 

* psij
    - JobSpec: executable, args, run directory, display name, environment vars, std in out & err paths, handle to ResourceSpec, 
        attributes (e.g. wall time), pre-launch script, post-launch script, handle to Launcher (e.g. mpirun) - there's an assumption here
        that there's a unix-like OS here, so its not "site specific" because only unix-like sites are supported 
    - ResourceSpec: #node, #proc, #gpu, etc. - (compute site specific)
    - Job: id, native id, handle to JobSpec, handle to on status change callback, handle to JobSpec, JobStatus (with own canonical states: 
        new, queued, active, completed, cancelled, failed, ), methods: set callback, wait, cancel

* lwfm 
    - JobDefn: display name, compute type ("resource", can also represent a sub-Site), 
        entry point ("executable", but not necessarily a path to an exe... we permit any 
        struct here and let Site.Run decide how to handle), 
        job args (any args to be interpreted by the Site.Run - different Sites might use these differently...) - 
        thus the JobDefn, like psij.ResourceSpec, is potentially Site specific 
    - RepoJobDefn: subclass of JobDefn to support data movement jobs 
    - JobStatus: job id, native job id, has a JobContext with parent & originator job ids, ref to to the site which produced this status
    - lwfm does not have its own "Job" class specifically - the Job fields from psij are in the lwfm.JobStatus. 
        psij also adds the status callback (github issue list suggests psij.Job.wait() doesn't work for all states)


(base) :~/src/lwfm-proj$ . lwfm/dev/start-mw.sh 


A -> B -> C use case (pseudocode)
    - synchronous: make & run A and wait for it, then repeat for B and then C
            # run job B after job A
            jobA = makeJob(A)
            jobA.submit()
            jobA.wait()
            # repeat for B and C
            jobB = makeJob(B)  etc.
    - asynchronous control flow: downstream jobs B and C may of may not have control and/or data flow dependencies; if 
        they have no control flow dependency, they can be run simultaneous.  if they have a control flow 
        dependency A triggers B.  
            jobA = makeJob(A)
            jobB = makeJob(B)
            setJobTrigger(jobB, jobA)
            jobA.submit()
    - asynchronous join: same as a control flow dependency, but B is dependent on a set of upstream A 
            jobA1 = makeJob(A)
            jobA2 = makeJob(A)
            jobB = makeJob(B)
            setJobTrigger(jobB, [jobA1, jobA2])
            jobA1.submit()
            jobA2.submit()
    - asynchronous data flow: if two jobs have a data flow dependency, the creation of data with a metadata
        signature (potenially but not necessarily by job A) triggers the running of B.
            


















* * * * * * * * * * * * * 

Questions: 
    - how important is long-term reproducibility vs. (meta)data storage 
    - nouns / verbs / building blocks backing up NLP wf constructs (wf fragments, w specific required input vars to be teased out) - "teach" them using what training corpus?
    - type 1, 2, 3 lens / unification 
    - multi-tool, multi-processor, multi-host, multi-lang fw, multi-lang



Middleware 
    * Type 1
        - MPMD controller (Python / C++) colaunches app(s) within the allocation, uses lwfm lib to (auth), 
            run other jobs via a site scheduler
        - if the controller is running a portable scheduler (e.g. flux) then the type 1 can use the lwfm lib
            to launch jobs vs a site which is the allocation itself
        - where is the middleware?  when does the controller need its own?  when does it use the invoking site's 
            type 2 middleware 
    * Type 2
        - enterprise middleware
    * Type 3 
        - personal middleware vs. enterprise middle running against permitted external sites - see auth, etc.


Use Cases

    1. lwfm middleware is deployed to a node within the enterprise and is scaled to support the size of the compute
        cluster (number of nodes, frequency of hits).  it supports jobs running on all HPC and other enterprise 
        servers.  
        
    1a. the enterprise middleware can be used for local user jobs 
    
    1b. the enterprise middleware can be used for type 3 workflows initiated from within an enterprise type 2 workflow.
    
    1c. the enterprise middleware can be used for type 3 workflows initiated locallty by the user.
    
    1d. type 3 workflows in which the remote job may call back on the enterprise middleware are permitted.
    
    1e. type 3 workflows in which the remote job may not call back on the enterprise middleware are only permitted.  here the remote site may have its own copy of the middleware, thus import/export is utilized.


    2. lwfm middleware is deployed by the user in the scope of user and other compute servers as if they were 
        their own enterprise - see use case 1.

    lwfm middleware can be deployed in lightweight singleton user or process-launched instances, or scaled 
    enterprise instances

    middleware: metarepo, event handler 
    interfaces: sdk, gui



C++ interface should have Repo method for fire & forget output annotion, Run interface to launch other jobs vs. scheduler, etc.


data movement jobs
jobs decorated with requirements, sites decorated with capabilities 



* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * 

TODO - KEEP 


+ now
    - test loggger in dt4d example
    - compute type in status message via job context (anything else to do?)
    - listComputeTypes returns complex list object w. metadata

lwfm: Repo.Run job status serialization incl. subclasses & job context

+ priority 1
    + event handlers
        - metadata-based filter implementation & example
        - better examples around multi-site - not all sites implement, but local does
        - triggers should work like >= as remote sites might not report all states
        - test: trigger job running on remote site, targer runs on remote site
    - SiteFileRef with timestamp (see TODO)
    - MetaRepo notate()


+ priority 2
    - JobStatus history
    - multi-Site Job status panel -> similar to DT4D's, but multi-Site
    - demo: lab device as a Site
    + event handlers
        - persisting over lwfm restart
        - full trigger model impl - fuzzy, timeouts, wildcards, persistence, etc.


+ priority 3
    - local site driver subclass with ssh as run, scp as repo, with auth credentials
    - digital thread graph render & navigate, incl. event trigger futures
    - RunRepo - robust impl
    - JSS security
    - adaptive job status polling based on emit & received job status timestamps




