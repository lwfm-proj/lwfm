
lwfm, the lightweight workflow manager, is a reference implementation of a 3-tier, 4-pillar workflow model in Python.  We admit, and have been called to task publicly at conferences, that the world does not need yet another workflow manager.  What it needs are standards for workflow interoperability (at least, punting on topics such as repeatability for the moment),and a standard interface for FAIR data and metadata management.  Workflow interop might include a design for a common job specification and indicators of a compute resource's capabilities, permitting automatic best fit.  

We have defined four points of interoperability - authentication, execution, storage, and provisioning.  Most workflow tools cover at least one of these, but not all.  Further, we define the interoperability at three levels: within a job allocation, within an enterprise, and across enterprises.  Each poses their own challenges relating to security, performance, and other architectural considerations.

If there was a framework which encompassed all four functional pillars in all three aspects, we would take an "if you can't beat them join them" approach.  We could consider taking an existing framework, and bolting for example FAIR data management to it, but we note that the integration is not a technical matter, but one of politics and community building.  Our experience in the workflow domain, as evidenced by the littered landscape of workflow tools, suggests that the political will doesn't exist.  

Suppose a library such as lwfm existed, fully formed, would it be used?  Unlikely in its present form - any interoperability is not free, in costs normally measured by performance either at runtime or during development.  So to gain adoption, especially with the scientific and engineering community, one must improve on performance, and no amount of hand-wringing over other architectural values will be sufficient.

Suppose there exists an NLP AI assistant which enabled performance improvements in workflow development time, with required runtime impact trending to zero (in other words, the thinnest of facades with the most minimal use case).   What if that workflow implementation was translatable, by the LLM, into the syntax required by the human user - be it Python, or something more graphical in nature.  Would we then care which workflow framework was being utilized, especially if their representations and results were transformable / interoperable?  Perhaps not.  Then upon which workflow framework might we base this AI assistant?  Upon one which was aware of all the functional pillars, not just runtime but also provenancial data management?

What follows are notes regarding yet another comparison of major competitors to lwfm.  Some years ago, before the maturation of our framework, we did a similar comparison but felt it timely to revisit some major alternatives again: Parsl and PSI/J specifically.  The ExaWorks project, with collaborators at the national labs, U. Chicago, etc. embodies both of these, making this organization, and the project, the ideal target for any future collaboration.  

The details of the comparison are in the following sections.  Let's begin with a review of the major classes in the lwfm model.

A Site is the top-level lwfm construct, and sites must provide for the four pillars, each represented by a sub-interface of Site.  Sites which utilize common infrastructure (the same vendor's HPC scheduler, for example) can reuse these components.  

In lwfm, JobStatus contains a discrete state from an enumeration of states (pending,cancelled, etc.).  The state changes over time and each change is noted at a certain timestamp - one when the state change happened in the compute environment, and another timestamp when the state change is received by the lwfm middleware. There is therefore a history in time of those state transitions.  The state enumeration is from the canonical lwfm states, so there's also (for non-"local" jobs) a native state, and its the duty of the Site.Run interface to perform that mapping between states.  This is similar to other prior and now defunct inter-site workflow systems like Balsam.  

The JobStatus also includes a set of fields grouped as a JobContext.  These include the job id assigned when the job was submitted to the runtime (Site.Run), and the lwfm id has a corresponding native runtime job id.  It has the lwfm job ids of this job's parent job, and the originator job of the whole workflow.  There is a job display name, and the name of the Site on which the job is queued.  If the Site exposes a specific named compute type (a specific configuration of a computing resource) it is noted here.  The JobContext is not reusable, and a JobStatus once emitted it immutable.  

A JobDefn defines the static job - what will be executed when the job is run, for example "run ansys with certain arguments".  And it names a symbolic resource - a compute type - on which the job must run.  An arbitrary args list is supported.  A subclass of JobDefn is the RepoJobDefn which is a convenience mechanism for data movement jobs.

An WorkflowEventTrigger is a rule which fires a registered JobDefn on a specific event or set of events.  For example, a job might be set to run when some other upstream job or jobs completes.  In lwfm, the triggering is enabled across sites, and for control flow and data flow event handling.  For example, a job might be set to run when data matching a certain pattern is available.

Thus in lwfm a "job" has two representations - one static (the JobDefn) and one dynamic (the JobContext with its running list of JobStatus).  The workflow also has static and dynamic representations, although the ad hoc programming nature of lwfm permits workflows which are defined on the fly and are alterable based on conditions.  Thus any runtime planning optimizations would need to be iterative.  

PSI/J is a framework intending to place an abstraction layer over common runtime schedulers (e.g. Condor, SLURM, etc.).

In PSI/J, a JobSpec is like an lwfm JobDefn - it contains the name of the executable, args, the run directory, display name, environment vars, std in out & err paths, a pre-launch script, a post-launch script, and other runtime boundary attributes such as duration (wall time), project name (for accounting), and a catch-all generic arg list.  There is a sub-object, the ResourceSpec, which defines the number of required compute nodes, how many GPUs, etc.  And it defines the MPI launcher (e.g. mpirun).  Thus the PSI/J JobSpec is site-specific - the launcher is site specific, as is the request for a specific node and GPU configuration.  PSI/J abstracts some but not all of the details, which impacts portability.  Some of this may be unavoidable in order to make all features of a cutting edge HPC system available to power users, but perhaps not as much as is allowed by PSI/J.

Comparing PSI/J JobSpec to the lwfm JobDefn, they are conceptually the same, with the lwfm arbitrary args list encompassing all the PSI/J args.  The lwfm compute type can be used as a symbolic name representing a set of attributes and their values - the Site can expose this as a specific named configuration, in addition to permitting user-defined arbitrary runtime  configurations. The PSI/J adds specific named fields for things like number of compute nodes, and it tacks down the launcher syntax.  lwfm does not expose the launcher syntax to the user - that is the purview of the Site.Run implementation, which of course is Site-specific, but it frees the lwfm JobDefn be more portable.  

In PSI/J, the Job object contains what in lwfm is the JobContext - the canonical and native job ids - and the lwfm JobStatus - the canonical job state.  The PSI/J Job object includes handles to the other PSI/J constructs such as JobSpec, but also includes a per-Job user-settable "on state change callback" method.  A callback of this kind might, for example, launch the next job in some A -> B control flow sequence.  The callback can also be used for error handling.  In lwfm, these functions are handled by the Event Trigger concept and a persistent middleware component with the interface to it exposed in Site.Run - hence in lwfm a JobDefn can be set to fire on the state change of some other job's execution and state changes.  

PSI/J only represents the Run portion of the lwfm Site model.  Thus as said, bolting lwfm to PSI/J would be a political activity.  However, bolting PSI/J under the hood of lwfm is entirely line-of-sight - the concepts map almost verbatim, and a Site.Run subsystem implemented with PSI/J makes a nice building block on which to model many real-world sites which use the runtime schedulers supported by PSI/J.  This is the direction we will take, for its expedience.  

As part of this investigation we also re-examined Parsl, a Python library with annotations for distributed applications which are written in either Python itself, or bash.  Parsl takes a data-flow centric approach built around files, and rather than implement an independently scalable middle tier like lwfm, Parsl distributes task management responsibility to every compute node in the allocation.  There is significant overlap in the runtime model of PSI/J and Parsl as they originate from the same community in and around Chicago.  The detailed notes of the Parsl project suggest that its a roadmap item to fold PSI/J under it - again, PSI/J by itself is only part of the functionality needed by site-distributed HPC apps.  

We also recently looked at Mojo as an alternative to Python, the former promising backward compatibility to Python with significant speed up, perhaps being useable in both computations and workflows.  Its simply not ready for prime time (e.g. doesn't implement kwargs).  Nor is Julia ready for programming end-to-end jobs, with the added problem that it requires code to be rewritten.  (See the Julia paper at SC23 for more details.)  We note that work at Argonne presented at SC23 to utilize an LLM to aid in workflow and workflow fragment programming first took a workflow expressed in Common Workflow Language mark-up, and rewrote it using Python before trying to present it to an LLM.  

Some summary of the packages is below:

Parsl 
    - parsl deployed on every HPC node 
    - parsl on head node doles out tasks to nodes based on file-based data flow 
    - python and bash apps supported
    - has own launcher / executor model - PSI/J driver coming soon?
    - distributed launchers on every node (compare vs lwfm centrally scaled)

PSI/J
    - JobSpec: cmd line, args, stdout & err 
    - JobAttributes: project id, etc.
    - ResourceSpec: nodes, processes
    - Executor: e.g. slurm 
    - Launcher: e.g. mpirun
    - Job: attach JobSpec & ResourceSpec, set launcher, submit & wait or set job status callback python func
    - Job Status: state (https://exaworks.org/psij-python/docs/v/0.9.3/.generated/tree.html#psij.JobState) + history
    - Job State = enum 
    - ProcessReaper - polls processes being tracked to completion, uses node list file to track deleted at end of run; starting the executor starts the singleton reaper 
    - typical coding scenario:
            make an Executor (e.g. slurm)
            make a JobSpec (executable, args, in/out)
            make ResourceSpec (e.g. #nodes)
            make a Job - set the JobSpec, ResourceSpec, launcher (e.g. mpirun)
            submit the job 
            sit on job.wait() 

* * * * * * 

The LLM summarizing our NAFEMS '23 paper and lwfm interface: 

"This research addresses the challenges of running engineering simulations across diverse computing systems. It introduces an architecture with three workflow types: in-situ, intra-site, and inter-site. This architecture simplifies interfaces, making workflows more accessible and enabling interoperability and traceability across heterogeneous systems. By focusing on interoperability rather than implementation details, the architecture allows diverse tools and systems to be integrated and used together, lowering barriers to entry and expanding the range of users and problems that can be addressed." key topics: interoperable workflows, data provenance, workflow architecture, heterogeneous computing, site interoperability "The simplicity of the site driver implementation is due to the fact that each of the four pillars is relatively self-contained and has a limited number of functions. This makes it easy to write the driver in a few hundred lines of code."

* * * * * * 

